{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcase some of the features of skorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces you to some of the nice features offered by [skorch](https://github.com/skorch-dev/skorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a companion notebook the PyCon/PyData Berlin 2019 presentation that can be found [here](https://github.com/BenjaminBossan/public-presentations/blob/master/20191010-pycon-pydata/presentation.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeds and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'  # choose 'cuda' or 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A toy binary classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(10000, 20, n_informative=10, random_state=0)\n",
    "X = X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 20), (10000,), 0.5003)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape, y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the PyTorch `module`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a vanilla neural network with one hidden layer. The output layer should have 2 output units since there are two classes. In addition, it should have a softmax nonlinearity, because later, when calling `predict_proba`, the output from the `forward` call will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_units=10, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(20, num_units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output = nn.Linear(num_units, 2)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = F.relu(self.dense(X))\n",
    "        X = self.dropout(X)\n",
    "        X = F.softmax(self.output(X), dim=-1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction of boilerplate code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure PyTorch implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show a basic training loop implemented with just PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "loader_train = DataLoader(ds_train, batch_size=256, shuffle=True)\n",
    "ds_valid = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))\n",
    "loader_valid = DataLoader(ds_valid, batch_size=256)\n",
    "module = MyModule().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(module.parameters(), lr=0.02)\n",
    "criterion = nn.NLLLoss()\n",
    "template = \"epoch: {} | loss train: {:.4f} | loss valid: {:.4f} | acc valid: {:.4f} | dur: {:.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | loss train: 0.6813 | loss valid: 0.6524 | acc valid: 0.5941 | dur: 0.226\n",
      "epoch: 2 | loss train: 0.6420 | loss valid: 0.6199 | acc valid: 0.6419 | dur: 0.116\n",
      "epoch: 3 | loss train: 0.6305 | loss valid: 0.6081 | acc valid: 0.6516 | dur: 0.089\n",
      "epoch: 4 | loss train: 0.6206 | loss valid: 0.6026 | acc valid: 0.6636 | dur: 0.090\n",
      "epoch: 5 | loss train: 0.6006 | loss valid: 0.5945 | acc valid: 0.6597 | dur: 0.084\n",
      "epoch: 6 | loss train: 0.5867 | loss valid: 0.5763 | acc valid: 0.6899 | dur: 0.089\n",
      "epoch: 7 | loss train: 0.5776 | loss valid: 0.5593 | acc valid: 0.6984 | dur: 0.084\n",
      "epoch: 8 | loss train: 0.5628 | loss valid: 0.5546 | acc valid: 0.7001 | dur: 0.090\n",
      "epoch: 9 | loss train: 0.5463 | loss valid: 0.5418 | acc valid: 0.7175 | dur: 0.091\n",
      "epoch: 10 | loss train: 0.5377 | loss valid: 0.5287 | acc valid: 0.7244 | dur: 0.089\n",
      "epoch: 11 | loss train: 0.5308 | loss valid: 0.5211 | acc valid: 0.7345 | dur: 0.090\n",
      "epoch: 12 | loss train: 0.5181 | loss valid: 0.5121 | acc valid: 0.7427 | dur: 0.095\n",
      "epoch: 13 | loss train: 0.5139 | loss valid: 0.5015 | acc valid: 0.7383 | dur: 0.090\n",
      "epoch: 14 | loss train: 0.5061 | loss valid: 0.4949 | acc valid: 0.7480 | dur: 0.129\n",
      "epoch: 15 | loss train: 0.5074 | loss valid: 0.5016 | acc valid: 0.7388 | dur: 0.089\n",
      "epoch: 16 | loss train: 0.4985 | loss valid: 0.4990 | acc valid: 0.7462 | dur: 0.085\n",
      "epoch: 17 | loss train: 0.4907 | loss valid: 0.5038 | acc valid: 0.7366 | dur: 0.092\n",
      "epoch: 18 | loss train: 0.4885 | loss valid: 0.4809 | acc valid: 0.7597 | dur: 0.090\n",
      "epoch: 19 | loss train: 0.4800 | loss valid: 0.4827 | acc valid: 0.7527 | dur: 0.090\n",
      "epoch: 20 | loss train: 0.4803 | loss valid: 0.4841 | acc valid: 0.7486 | dur: 0.094\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    tic = time.time()\n",
    "    losses_train = []\n",
    "    for Xb, yb in loader_train:\n",
    "        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "        y_proba = module(Xb)\n",
    "        loss = criterion(torch.log(y_proba), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses_train.append(loss.item())\n",
    "        \n",
    "    losses_valid = []\n",
    "    accuracy_valid = []\n",
    "    for Xb, yb in loader_valid:\n",
    "        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "        y_proba = module(Xb)\n",
    "        loss = criterion(torch.log(y_proba), yb)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses_valid.append(loss.item())\n",
    "        accuracy_valid.append(accuracy_score(yb.cpu().numpy(), y_proba.argmax(1).cpu().numpy()))\n",
    "        \n",
    "    toc = time.time() - tic\n",
    "    print(template.format(\n",
    "        epoch + 1, np.mean(losses_train), np.mean(losses_valid), np.mean(accuracy_valid), toc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with skorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show how to achieve the same outcome with skorch. Note how we don't need to make any adjustments to the `module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    MyModule,\n",
    "    module__num_units=50,\n",
    "    max_epochs=20,\n",
    "    lr=0.02,\n",
    "    batch_size=256,\n",
    "    iterator_train__shuffle=True,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6981\u001b[0m       \u001b[32m0.6992\u001b[0m        \u001b[35m0.6096\u001b[0m  0.1066\n",
      "      2        \u001b[36m0.6155\u001b[0m       \u001b[32m0.7441\u001b[0m        \u001b[35m0.5650\u001b[0m  0.1433\n",
      "      3        \u001b[36m0.5799\u001b[0m       \u001b[32m0.7671\u001b[0m        \u001b[35m0.5319\u001b[0m  0.1370\n",
      "      4        \u001b[36m0.5485\u001b[0m       \u001b[32m0.7871\u001b[0m        \u001b[35m0.5050\u001b[0m  0.1291\n",
      "      5        \u001b[36m0.5321\u001b[0m       \u001b[32m0.8001\u001b[0m        \u001b[35m0.4833\u001b[0m  0.1281\n",
      "      6        \u001b[36m0.5163\u001b[0m       \u001b[32m0.8101\u001b[0m        \u001b[35m0.4649\u001b[0m  0.1103\n",
      "      7        \u001b[36m0.4959\u001b[0m       \u001b[32m0.8226\u001b[0m        \u001b[35m0.4465\u001b[0m  0.1231\n",
      "      8        \u001b[36m0.4869\u001b[0m       \u001b[32m0.8341\u001b[0m        \u001b[35m0.4312\u001b[0m  0.1400\n",
      "      9        \u001b[36m0.4721\u001b[0m       \u001b[32m0.8421\u001b[0m        \u001b[35m0.4180\u001b[0m  0.1423\n",
      "     10        \u001b[36m0.4663\u001b[0m       \u001b[32m0.8481\u001b[0m        \u001b[35m0.4052\u001b[0m  0.1434\n",
      "     11        \u001b[36m0.4455\u001b[0m       \u001b[32m0.8541\u001b[0m        \u001b[35m0.3937\u001b[0m  0.1077\n",
      "     12        \u001b[36m0.4395\u001b[0m       \u001b[32m0.8581\u001b[0m        \u001b[35m0.3826\u001b[0m  0.1210\n",
      "     13        \u001b[36m0.4311\u001b[0m       \u001b[32m0.8616\u001b[0m        \u001b[35m0.3726\u001b[0m  0.1277\n",
      "     14        \u001b[36m0.4250\u001b[0m       \u001b[32m0.8671\u001b[0m        \u001b[35m0.3641\u001b[0m  0.1078\n",
      "     15        \u001b[36m0.4176\u001b[0m       \u001b[32m0.8696\u001b[0m        \u001b[35m0.3569\u001b[0m  0.1387\n",
      "     16        \u001b[36m0.4095\u001b[0m       \u001b[32m0.8746\u001b[0m        \u001b[35m0.3491\u001b[0m  0.1148\n",
      "     17        \u001b[36m0.4052\u001b[0m       \u001b[32m0.8791\u001b[0m        \u001b[35m0.3421\u001b[0m  0.1299\n",
      "     18        \u001b[36m0.3982\u001b[0m       \u001b[32m0.8816\u001b[0m        \u001b[35m0.3354\u001b[0m  0.1289\n",
      "     19        \u001b[36m0.3884\u001b[0m       \u001b[32m0.8831\u001b[0m        \u001b[35m0.3297\u001b[0m  0.1137\n",
      "     20        \u001b[36m0.3833\u001b[0m       \u001b[32m0.8851\u001b[0m        \u001b[35m0.3240\u001b[0m  0.1188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compatibility with sklearn API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor sklearn metrics during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EpochScoring\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = EpochScoring(\n",
    "    scoring=roc_auc_score,  # <-- just passing 'roc_auc' would also work\n",
    "    lower_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    MyModule,\n",
    "    module__num_units=50,\n",
    "    max_epochs=20,\n",
    "    lr=0.02,\n",
    "    batch_size=256,\n",
    "    iterator_train__shuffle=True,\n",
    "    device=DEVICE,\n",
    "    callbacks=[auc],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    roc_auc_score    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ---------------  ------------  -----------  ------------  ------\n",
      "      1           \u001b[36m0.6527\u001b[0m        \u001b[32m0.7091\u001b[0m       \u001b[35m0.6527\u001b[0m        \u001b[31m0.6307\u001b[0m  0.1310\n",
      "      2           \u001b[36m0.7216\u001b[0m        \u001b[32m0.6320\u001b[0m       \u001b[35m0.7216\u001b[0m        \u001b[31m0.5851\u001b[0m  0.1100\n",
      "      3           \u001b[36m0.7571\u001b[0m        \u001b[32m0.5908\u001b[0m       \u001b[35m0.7571\u001b[0m        \u001b[31m0.5520\u001b[0m  0.1362\n",
      "      4           \u001b[36m0.7741\u001b[0m        \u001b[32m0.5670\u001b[0m       \u001b[35m0.7741\u001b[0m        \u001b[31m0.5248\u001b[0m  0.1348\n",
      "      5           \u001b[36m0.7896\u001b[0m        \u001b[32m0.5467\u001b[0m       \u001b[35m0.7896\u001b[0m        \u001b[31m0.5016\u001b[0m  0.1266\n",
      "      6           \u001b[36m0.8086\u001b[0m        \u001b[32m0.5273\u001b[0m       \u001b[35m0.8086\u001b[0m        \u001b[31m0.4800\u001b[0m  0.1219\n",
      "      7           \u001b[36m0.8161\u001b[0m        \u001b[32m0.5090\u001b[0m       \u001b[35m0.8161\u001b[0m        \u001b[31m0.4614\u001b[0m  0.1170\n",
      "      8           \u001b[36m0.8221\u001b[0m        \u001b[32m0.4929\u001b[0m       \u001b[35m0.8221\u001b[0m        \u001b[31m0.4450\u001b[0m  0.1462\n",
      "      9           \u001b[36m0.8296\u001b[0m        \u001b[32m0.4874\u001b[0m       \u001b[35m0.8296\u001b[0m        \u001b[31m0.4305\u001b[0m  0.1189\n",
      "     10           \u001b[36m0.8371\u001b[0m        \u001b[32m0.4708\u001b[0m       \u001b[35m0.8371\u001b[0m        \u001b[31m0.4160\u001b[0m  0.1183\n",
      "     11           \u001b[36m0.8451\u001b[0m        \u001b[32m0.4642\u001b[0m       \u001b[35m0.8451\u001b[0m        \u001b[31m0.4035\u001b[0m  0.1219\n",
      "     12           \u001b[36m0.8486\u001b[0m        \u001b[32m0.4489\u001b[0m       \u001b[35m0.8486\u001b[0m        \u001b[31m0.3927\u001b[0m  0.1106\n",
      "     13           \u001b[36m0.8516\u001b[0m        \u001b[32m0.4370\u001b[0m       \u001b[35m0.8516\u001b[0m        \u001b[31m0.3817\u001b[0m  0.1209\n",
      "     14           \u001b[36m0.8596\u001b[0m        \u001b[32m0.4301\u001b[0m       \u001b[35m0.8596\u001b[0m        \u001b[31m0.3718\u001b[0m  0.1176\n",
      "     15           \u001b[36m0.8661\u001b[0m        \u001b[32m0.4237\u001b[0m       \u001b[35m0.8661\u001b[0m        \u001b[31m0.3631\u001b[0m  0.1486\n",
      "     16           \u001b[36m0.8665\u001b[0m        0.4245       \u001b[35m0.8666\u001b[0m        \u001b[31m0.3561\u001b[0m  0.1097\n",
      "     17           \u001b[36m0.8706\u001b[0m        \u001b[32m0.4056\u001b[0m       \u001b[35m0.8706\u001b[0m        \u001b[31m0.3492\u001b[0m  0.1260\n",
      "     18           \u001b[36m0.8725\u001b[0m        \u001b[32m0.4027\u001b[0m       \u001b[35m0.8726\u001b[0m        \u001b[31m0.3421\u001b[0m  0.1383\n",
      "     19           \u001b[36m0.8745\u001b[0m        \u001b[32m0.4008\u001b[0m       \u001b[35m0.8746\u001b[0m        \u001b[31m0.3358\u001b[0m  0.1381\n",
      "     20           \u001b[36m0.8810\u001b[0m        \u001b[32m0.3898\u001b[0m       \u001b[35m0.8811\u001b[0m        \u001b[31m0.3281\u001b[0m  0.1297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support for the basic methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = net.predict(X[:5])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75151503, 0.24848492],\n",
       "       [0.15232019, 0.8476798 ],\n",
       "       [0.18867724, 0.81132275],\n",
       "       [0.5423922 , 0.45760784],\n",
       "       [0.40528426, 0.5947157 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = net.predict_proba(X[:5])\n",
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_params();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.set_params(verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = clone(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinh/anaconda3/envs/skorch/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([1.9804399 , 1.77662516, 1.96318698]),\n",
       " 'score_time': array([0.03349638, 0.03385663, 0.03398347]),\n",
       " 'test_score': array([0.86502699, 0.87822436, 0.8772509 ]),\n",
       " 'train_score': array([0.86843684, 0.87983798, 0.87177564])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(net, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use inside an sklearn `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('net', <class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       "))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('net', net),\n",
    "])\n",
    "\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50151825, 0.49848178],\n",
       "       [0.25543442, 0.7445656 ],\n",
       "       [0.37828928, 0.62171066],\n",
       "       [0.5110312 , 0.48896876],\n",
       "       [0.40677232, 0.5932276 ]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict_proba(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the whole pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the whole pipeline, including preprocessing and the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinh/anaconda3/envs/skorch/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type MyModule. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "with open('my_pipeline.pickle', 'wb') as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No special adjustments need to be made to perform a hyperparameter search on the net parameters. We can even search on the `__init__` parameters of our `module` by using the `'module__'` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_epochs': [10, 20],\n",
    "    'optimizer__momentum': [0.0, 0.9],\n",
    "    'module__num_units': [10, 50],  # <-- just works\n",
    "    'module__dropout': [0, 0.5],  # <-- just works\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.0, total=   1.1s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.0, total=   0.9s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.0, total=   1.0s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.9, total=   1.0s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.9, total=   1.1s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.9, total=   1.0s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.0, total=   1.1s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.0, total=   1.1s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.0, total=   1.0s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.9, total=   0.9s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.9, total=   0.9s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.9, total=   0.9s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0, total=   0.9s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0, total=   0.9s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0, total=   0.9s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9, total=   1.0s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9, total=   0.9s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9, total=   1.0s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0, total=   1.0s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0, total=   1.1s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0, total=   0.9s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9, total=   1.0s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9, total=   0.9s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9, total=   1.2s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.0, total=   2.3s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.0, total=   2.6s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.0, total=   2.2s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.9, total=   2.3s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.9, total=   2.1s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.9, total=   2.1s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.0, total=   2.0s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.0, total=   2.3s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.0, total=   2.3s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.9, total=   2.0s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.9, total=   1.8s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.9, total=   2.0s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0, total=   2.1s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0, total=   1.9s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0, total=   2.0s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9, total=   2.1s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9, total=   2.0s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9, total=   1.9s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0, total=   2.0s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0, total=   2.1s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0, total=   2.1s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9, total=   2.2s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9, total=   2.0s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9, total=   2.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 9s, sys: 16.7 s, total: 5min 26s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%time search = GridSearchCV(net, params, verbose=2, cv=3).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9565,\n",
       " {'max_epochs': 20,\n",
       "  'module__dropout': 0,\n",
       "  'module__num_units': 50,\n",
       "  'optimizer__momentum': 0.9})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_, search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can grid search the parameters of almost everything:\n",
    "\n",
    "- NeuralNet\n",
    "- module\n",
    "- optimizer\n",
    "- criterion\n",
    "- DataLoader\n",
    "- callbacks\n",
    "\n",
    "Just use the `__` notation known from sklearn, e.g. `optimizer__momentum` to set the `momentum` parameter of the optimizer. To make a search on callback parameters, give the parameter a name and use that name to dispatch to the callback. E.g.:\n",
    "\n",
    "```\n",
    "net = NeuralNetClassifier(..., callbacks=[('mycb', MyCallback(foo=1))])\n",
    "params = {'callbacks__mycb__foo': [1, 2, 3]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swap skorch net for any other sklearn estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since skorch's estimators work like any other sklearn estimator, you can swap them out to see which one leads to the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare our neural network with a logistic regression and a KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.set_params(**search.best_params_)  # use the best parameters from grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('model', net),\n",
    "])\n",
    "params = {'model': [net, LogisticRegression(), KNeighborsClassifier()]}\n",
    "search = GridSearchCV(pipe, params, verbose=2, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] model=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
      "  module_=MyModule(\n",
      "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0)\n",
      "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
      "  ),\n",
      ") \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  model=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
      "  module_=MyModule(\n",
      "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0)\n",
      "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
      "  ),\n",
      "), total=   2.5s\n",
      "[CV] model=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
      "  module_=MyModule(\n",
      "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0)\n",
      "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
      "  ),\n",
      ") \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  model=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
      "  module_=MyModule(\n",
      "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0)\n",
      "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
      "  ),\n",
      "), total=   2.5s\n",
      "[CV] model=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
      "  module_=MyModule(\n",
      "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0)\n",
      "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
      "  ),\n",
      ") \n",
      "[CV]  model=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
      "  module_=MyModule(\n",
      "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0)\n",
      "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
      "  ),\n",
      "), total=   2.1s\n",
      "[CV] model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) \n",
      "[CV]  model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), total=   0.0s\n",
      "[CV] model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) \n",
      "[CV]  model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), total=   0.0s\n",
      "[CV] model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) \n",
      "[CV]  model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), total=   0.0s\n",
      "[CV] model=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinh/anaconda3/envs/skorch/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/vinh/anaconda3/envs/skorch/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/vinh/anaconda3/envs/skorch/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  model=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'), total=   0.8s\n",
      "[CV] model=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') \n",
      "[CV]  model=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'), total=   0.8s\n",
      "[CV] model=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') \n",
      "[CV]  model=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'), total=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   14.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.3 s, sys: 2.06 s, total: 48.3 s\n",
      "Wall time: 16.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', <class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       "))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'model': [<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       "), LogisticRegression(C=1.0, class_...ki',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9491,\n",
       " {'model': <class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "    module_=MyModule(\n",
       "      (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "    ),\n",
       "  )})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_, search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distributed `GridSearchCV` with dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a distributed hyperparameter search, you need `dask` and `dask.distributed`:\n",
    "\n",
    "`$ pip install dask distributed`\n",
    "\n",
    "Setup your dask workers as described [here](https://docs.dask.org/en/latest/setup.html).\n",
    "\n",
    "Then run the following lines:\n",
    "\n",
    "```\n",
    "from dask.distributed import Client\n",
    "from joblib import parallel_backend\n",
    "\n",
    "client = Client('127.0.0.1:8786')\n",
    "\n",
    "search = GridSearchCV(net, params, verbose=2, cv=3)\n",
    "\n",
    "with parallel_backend('dask'):\n",
    "    search.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More additions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the `state_dict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to save the `state_dict` of our module (and maybe our optimizer), we can either use the `Checkpoint` callback or call the `save_params` method. Use `load_params` to load the `state_dict` later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = Checkpoint(monitor='valid_loss_best', dirname='exp1')\n",
    "net = NeuralNetClassifier(\n",
    "    MyModule,\n",
    "    module__num_units=50,\n",
    "    max_epochs=20,\n",
    "    lr=0.02,\n",
    "    batch_size=256,\n",
    "    iterator_train__shuffle=True,\n",
    "    device=DEVICE,\n",
    "    callbacks=[cp],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m0.6848\u001b[0m       \u001b[32m0.6957\u001b[0m        \u001b[35m0.5929\u001b[0m     +  0.1037\n",
      "      2        \u001b[36m0.5999\u001b[0m       \u001b[32m0.7401\u001b[0m        \u001b[35m0.5464\u001b[0m     +  0.0993\n",
      "      3        \u001b[36m0.5626\u001b[0m       \u001b[32m0.7686\u001b[0m        \u001b[35m0.5148\u001b[0m     +  0.0991\n",
      "      4        \u001b[36m0.5356\u001b[0m       \u001b[32m0.7851\u001b[0m        \u001b[35m0.4899\u001b[0m     +  0.0962\n",
      "      5        \u001b[36m0.5129\u001b[0m       \u001b[32m0.8006\u001b[0m        \u001b[35m0.4678\u001b[0m     +  0.0962\n",
      "      6        \u001b[36m0.4924\u001b[0m       \u001b[32m0.8126\u001b[0m        \u001b[35m0.4470\u001b[0m     +  0.0970\n",
      "      7        \u001b[36m0.4868\u001b[0m       \u001b[32m0.8216\u001b[0m        \u001b[35m0.4311\u001b[0m     +  0.1022\n",
      "      8        \u001b[36m0.4708\u001b[0m       \u001b[32m0.8281\u001b[0m        \u001b[35m0.4149\u001b[0m     +  0.1355\n",
      "      9        \u001b[36m0.4513\u001b[0m       \u001b[32m0.8396\u001b[0m        \u001b[35m0.3999\u001b[0m     +  0.1009\n",
      "     10        \u001b[36m0.4426\u001b[0m       \u001b[32m0.8486\u001b[0m        \u001b[35m0.3868\u001b[0m     +  0.0993\n",
      "     11        \u001b[36m0.4368\u001b[0m       \u001b[32m0.8596\u001b[0m        \u001b[35m0.3752\u001b[0m     +  0.0987\n",
      "     12        \u001b[36m0.4346\u001b[0m       \u001b[32m0.8631\u001b[0m        \u001b[35m0.3654\u001b[0m     +  0.0991\n",
      "     13        \u001b[36m0.4226\u001b[0m       \u001b[32m0.8696\u001b[0m        \u001b[35m0.3554\u001b[0m     +  0.1096\n",
      "     14        \u001b[36m0.4116\u001b[0m       \u001b[32m0.8736\u001b[0m        \u001b[35m0.3465\u001b[0m     +  0.0959\n",
      "     15        \u001b[36m0.4098\u001b[0m       \u001b[32m0.8771\u001b[0m        \u001b[35m0.3394\u001b[0m     +  0.1215\n",
      "     16        \u001b[36m0.3951\u001b[0m       \u001b[32m0.8811\u001b[0m        \u001b[35m0.3311\u001b[0m     +  0.1044\n",
      "     17        \u001b[36m0.3939\u001b[0m       \u001b[32m0.8856\u001b[0m        \u001b[35m0.3242\u001b[0m     +  0.0958\n",
      "     18        \u001b[36m0.3897\u001b[0m       \u001b[32m0.8901\u001b[0m        \u001b[35m0.3169\u001b[0m     +  0.1013\n",
      "     19        \u001b[36m0.3747\u001b[0m       \u001b[32m0.8921\u001b[0m        \u001b[35m0.3101\u001b[0m     +  0.0973\n",
      "     20        0.3803       \u001b[32m0.8951\u001b[0m        \u001b[35m0.3048\u001b[0m     +  0.0994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, y)  # Checkpoint saves each time valid lost improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_params(\n",
    "    f_params='exp1/mynet.pt',  # <- state dict of module\n",
    "    f_optimizer='exp1/myoptimizer.pt',  # <- state dict of optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling of different data formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, skorch handles the most common data formats, even more complex ones like dictionaries. If this doesn't fit your need, just define your own `Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- numpy arrays\n",
    "- PyTorch Datasets (most)\n",
    "- dict or list of arrays\n",
    "- pandas DataFrames\n",
    "- scipy sparse CSR matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skorch comes packaged with a few useful callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import GradientNormClipping\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.callbacks import EpochScoring, BatchScoring\n",
    "from skorch.callbacks import Checkpoint, TrainEndCheckpoint, LoadInitState\n",
    "from skorch.callbacks import Freezer\n",
    "from skorch.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of skorch and Google's fire library, it is exceedingly easy to transform your training script into a nice CLI. This is what skorch and fire will automatically take care of:\n",
    "\n",
    "* help for the CLI usage\n",
    "* show docstrings in CLI help\n",
    "* set __all__ possible parameters from the command line without any manuel argument parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install fire and numpydoc:\n",
    "    \n",
    "`$ pip install fire numpydoc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It requires only a few lines of code to turn your script into a nice CLI:\n",
    "\n",
    "```\n",
    "def main(..., **kwargs):\n",
    "    model = ...  # put model definition here\n",
    "\n",
    "    model = parse_args(kwargs)(model)  # <-- add this line\n",
    "    \n",
    "    model.fit(X, y)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(main)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the complete train.py script. Note the few lines that needed to be added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Simple training script for a MLP classifier.\r\n",
      "\r\n",
      "See accompanying `pycon_showcase_skorch.ipynb` for details.\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "import pickle\r\n",
      "\r\n",
      "import fire\r\n",
      "import numpy as np\r\n",
      "from sklearn.datasets import make_classification\r\n",
      "from skorch import NeuralNetClassifier\r\n",
      "import torch\r\n",
      "from torch import nn\r\n",
      "\r\n",
      "from skorch.helper import parse_args\r\n",
      "\r\n",
      "\r\n",
      "np.random.seed(0)\r\n",
      "torch.manual_seed(0)\r\n",
      "torch.cuda.manual_seed(0)\r\n",
      "\r\n",
      "\r\n",
      "# number of input features\r\n",
      "N_FEATURES = 20\r\n",
      "\r\n",
      "# number of classes\r\n",
      "N_CLASSES = 2\r\n",
      "\r\n",
      "# custom defaults for net\r\n",
      "DEFAULTS = {\r\n",
      "    'batch_size': 256,\r\n",
      "    'module__hidden_units': 30,\r\n",
      "}\r\n",
      "\r\n",
      "\r\n",
      "class MLPClassifier(nn.Module):\r\n",
      "    \"\"\"A simple multi-layer perceptron module.\r\n",
      "\r\n",
      "    This can be adapted for usage in different contexts, e.g. binary\r\n",
      "    and multi-class classification, regression, etc.\r\n",
      "\r\n",
      "    Note: This docstring is used to create the help for the CLI.\r\n",
      "\r\n",
      "    Parameters\r\n",
      "    ----------\r\n",
      "    hidden_units : int (default=10)\r\n",
      "      Number of units in hidden layers.\r\n",
      "\r\n",
      "    num_hidden : int (default=1)\r\n",
      "      Number of hidden layers.\r\n",
      "\r\n",
      "    nonlin : torch.nn.Module instance (default=torch.nn.ReLU())\r\n",
      "      Non-linearity to apply after hidden layers.\r\n",
      "\r\n",
      "    dropout : float (default=0)\r\n",
      "      Dropout rate. Dropout is applied between layers.\r\n",
      "\r\n",
      "    \"\"\"\r\n",
      "    def __init__(\r\n",
      "            self,\r\n",
      "            hidden_units=10,\r\n",
      "            num_hidden=1,\r\n",
      "            nonlin=nn.ReLU(),\r\n",
      "            dropout=0,\r\n",
      "    ):\r\n",
      "        super().__init__()\r\n",
      "        self.hidden_units = hidden_units\r\n",
      "        self.num_hidden = num_hidden\r\n",
      "        self.nonlin = nonlin\r\n",
      "        self.dropout = dropout\r\n",
      "\r\n",
      "        self.reset_params()\r\n",
      "\r\n",
      "    def reset_params(self):\r\n",
      "        \"\"\"(Re)set all parameters.\"\"\"\r\n",
      "        units = [N_FEATURES]\r\n",
      "        units += [self.hidden_units] * self.num_hidden\r\n",
      "        units += [N_CLASSES]\r\n",
      "\r\n",
      "        sequence = []\r\n",
      "        for u0, u1 in zip(units, units[1:]):\r\n",
      "            sequence.append(nn.Linear(u0, u1))\r\n",
      "            sequence.append(self.nonlin)\r\n",
      "            sequence.append(nn.Dropout(self.dropout))\r\n",
      "\r\n",
      "        sequence = sequence[:-2]\r\n",
      "        self.sequential = nn.Sequential(*sequence)\r\n",
      "\r\n",
      "    def forward(self, X):\r\n",
      "        return nn.Softmax(dim=-1)(self.sequential(X))\r\n",
      "\r\n",
      "\r\n",
      "def get_data(n_samples=100):\r\n",
      "    \"\"\"Get synthetic classification data with n_samples samples.\"\"\"\r\n",
      "    X, y = make_classification(\r\n",
      "        n_samples=n_samples,\r\n",
      "        n_features=N_FEATURES,\r\n",
      "        n_classes=N_CLASSES,\r\n",
      "        random_state=0,\r\n",
      "    )\r\n",
      "    X = X.astype(np.float32)\r\n",
      "    return X, y\r\n",
      "\r\n",
      "\r\n",
      "def save_model(model, output_file):\r\n",
      "    \"\"\"Save model to output_file, if given\"\"\"\r\n",
      "    if not output_file:\r\n",
      "        return\r\n",
      "\r\n",
      "    with open(output_file, 'wb') as f:\r\n",
      "        pickle.dump(model, f)\r\n",
      "    print(\"Saved model to file '{}'.\".format(output_file))\r\n",
      "\r\n",
      "\r\n",
      "def main(n_samples=100, output_file=None, **kwargs):\r\n",
      "    \"\"\"Train an MLP classifier on synthetic data.\r\n",
      "\r\n",
      "    n_samples : int (default=100)\r\n",
      "      Number of training samples\r\n",
      "\r\n",
      "    output_file : str (default=None)\r\n",
      "      If not None, file name used to save the model.\r\n",
      "\r\n",
      "    kwargs : dict\r\n",
      "      Additional model parameters.\r\n",
      "\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    model = NeuralNetClassifier(MLPClassifier)\r\n",
      "    # important: wrap the model with the parsed arguments\r\n",
      "    parsed = parse_args(kwargs, defaults=DEFAULTS)\r\n",
      "    model = parsed(model)\r\n",
      "\r\n",
      "    X, y = get_data(n_samples=n_samples)\r\n",
      "    print(\"Training MLP classifier\")\r\n",
      "    model.fit(X, y)\r\n",
      "\r\n",
      "    save_model(model, output_file)\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    fire.Fire(main)\r\n"
     ]
    }
   ],
   "source": [
    "!cat train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNAME\u001b[0m\r\n",
      "    train.py - Train an MLP classifier on synthetic data.\r\n",
      "\r\n",
      "\u001b[1mSYNOPSIS\u001b[0m\r\n",
      "    train.py <flags>\r\n",
      "\r\n",
      "\u001b[1mDESCRIPTION\u001b[0m\r\n",
      "    n_samples : int (default=100)\r\n",
      "      Number of training samples\r\n",
      "\r\n",
      "    output_file : str (default=None)\r\n",
      "      If not None, file name used to save the model.\r\n",
      "\r\n",
      "    kwargs : dict\r\n",
      "      Additional model parameters.\r\n",
      "\r\n",
      "\u001b[1mFLAGS\u001b[0m\r\n",
      "    --n_samples=\u001b[4mN_SAMPLES\u001b[0m\r\n",
      "    --output_file=\u001b[4mOUTPUT_FILE\u001b[0m\r\n",
      "    Additional flags are accepted.\r\n"
     ]
    }
   ],
   "source": [
    "!python train.py -- --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-specific help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the help for the model-specific parameters.\r\n",
      "To invoke help for the remaining options, run:\r\n",
      "python train.py -- --help\r\n",
      "\r\n",
      "<NeuralNetClassifier> options:\r\n",
      "  --module : torch module (class or instance)\r\n",
      "    A PyTorch :class:`~torch.nn.Module`. In general, the\r\n",
      "    uninstantiated class should be passed, although instantiated\r\n",
      "    modules will also work.\r\n",
      "  --criterion : torch criterion (class, default=torch.nn.NLLLoss)\r\n",
      "    Negative log likelihood loss. Note that the module should return\r\n",
      "    probabilities, the log is applied during ``get_loss``.\r\n",
      "  --optimizer : torch optim (class, default=torch.optim.SGD)\r\n",
      "    The uninitialized optimizer (update rule) used to optimize the\r\n",
      "    module\r\n",
      "  --lr : float (default=0.01)\r\n",
      "    Learning rate passed to the optimizer. You may use ``lr`` instead\r\n",
      "    of using ``optimizer__lr``, which would result in the same outcome.\r\n",
      "  --max_epochs : int (default=10)\r\n",
      "    The number of epochs to train for each ``fit`` call. Note that you\r\n",
      "    may keyboard-interrupt training at any time.\r\n",
      "  --batch_size : int (default=256)\r\n",
      "    Mini-batch size. Use this instead of setting\r\n",
      "    ``iterator_train__batch_size`` and ``iterator_test__batch_size``,\r\n",
      "    which would result in the same outcome. If ``batch_size`` is -1,\r\n",
      "    a single batch with all the data will be used during training\r\n",
      "    and validation.\r\n",
      "  --iterator_train : torch DataLoader\r\n",
      "    The default PyTorch :class:`~torch.utils.data.DataLoader` used for\r\n",
      "    training data.\r\n",
      "  --iterator_valid : torch DataLoader\r\n",
      "    The default PyTorch :class:`~torch.utils.data.DataLoader` used for\r\n",
      "    validation and test data, i.e. during inference.\r\n",
      "  --dataset : torch Dataset (default=skorch.dataset.Dataset)\r\n",
      "    The dataset is necessary for the incoming data to work with\r\n",
      "    pytorch's ``DataLoader``. It has to implement the ``__len__`` and\r\n",
      "    ``__getitem__`` methods. The provided dataset should be capable of\r\n",
      "    dealing with a lot of data types out of the box, so only change\r\n",
      "    this if your data is not supported. You should generally pass the\r\n",
      "    uninitialized ``Dataset`` class and define additional arguments to\r\n",
      "    X and y by prefixing them with ``dataset__``. It is also possible\r\n",
      "    to pass an initialzed ``Dataset``, in which case no additional\r\n",
      "    arguments may be passed.\r\n",
      "  --train_split : None or callable (default=skorch.dataset.CVSplit(5))\r\n",
      "    If None, there is no train/validation split. Else, train_split\r\n",
      "    should be a function or callable that is called with X and y\r\n",
      "    data and should return the tuple ``dataset_train, dataset_valid``.\r\n",
      "    The validation data may be None.\r\n",
      "  --callbacks : None or list of Callback instances (default=None)\r\n",
      "    More callbacks, in addition to those returned by\r\n",
      "    ``get_default_callbacks``. Each callback should inherit from\r\n",
      "    :class:`.Callback`. If not ``None``, a list of callbacks is\r\n",
      "    expected where the callback names are inferred from the class\r\n",
      "    name. Name conflicts are resolved by appending a count suffix\r\n",
      "    starting with 1, e.g. ``EpochScoring_1``. Alternatively,\r\n",
      "    a tuple ``(name, callback)`` can be passed, where ``name``\r\n",
      "    should be unique. Callbacks may or may not be instantiated.\r\n",
      "    The callback name can be used to set parameters on specific\r\n",
      "    callbacks (e.g., for the callback with name ``'print_log'``, use\r\n",
      "    ``net.set_params(callbacks__print_log__keys_ignored=['epoch',\r\n",
      "    'train_loss'])``).\r\n",
      "  --warm_start : bool (default=False)\r\n",
      "    Whether each fit call should lead to a re-initialization of the\r\n",
      "    module (cold start) or whether the module should be trained\r\n",
      "    further (warm start).\r\n",
      "  --verbose : int (default=1)\r\n",
      "    Control the verbosity level.\r\n",
      "  --device : str, torch.device (default='cpu')\r\n",
      "    The compute device to be used. If set to 'cuda', data in torch\r\n",
      "    tensors will be pushed to cuda tensors before being sent to the\r\n",
      "    module.\r\n",
      "\r\n",
      "<MLPClassifier> options:\r\n",
      "  --module__hidden_units : int (default=30)\r\n",
      "    Number of units in hidden layers.\r\n",
      "  --module__num_hidden : int (default=1)\r\n",
      "    Number of hidden layers.\r\n",
      "  --module__nonlin : torch.nn.Module instance (default=torch.nn.ReLU())\r\n",
      "    Non-linearity to apply after hidden layers.\r\n",
      "  --module__dropout : float (default=0)\r\n",
      "    Dropout rate. Dropout is applied between layers.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python train.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you can call the script from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP classifier\n",
      "  epoch    train_acc    train_loss    valid_loss     dur\n",
      "-------  -----------  ------------  ------------  ------\n",
      "      1       \u001b[36m0.7772\u001b[0m        \u001b[32m0.5813\u001b[0m        \u001b[35m0.5013\u001b[0m  0.1235\n",
      "      2       \u001b[36m0.9024\u001b[0m        \u001b[32m0.4874\u001b[0m        \u001b[35m0.4314\u001b[0m  0.0175\n",
      "      3       \u001b[36m0.9224\u001b[0m        \u001b[32m0.4195\u001b[0m        \u001b[35m0.3786\u001b[0m  0.0165\n",
      "      4       \u001b[36m0.9274\u001b[0m        \u001b[32m0.3670\u001b[0m        \u001b[35m0.3388\u001b[0m  0.0149\n",
      "      5       \u001b[36m0.9324\u001b[0m        \u001b[32m0.3259\u001b[0m        \u001b[35m0.3079\u001b[0m  0.0108\n",
      "Saved model to file 'exp1/model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "!python train.py --n_samples 1000 --output_file 'exp1/model.pkl' --lr 0.1 --max_epochs 5 \\\n",
    "  --device 'cuda' --module__hidden_units 50 --module__nonlin 'torch.nn.RReLU(0.1, upper=0.4)'\\\n",
    "  --callbacks__valid_acc__on_train --callbacks__valid_acc__name train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how you can even pass Python objects as arguments like `--module__nonlin 'torch.nn.RReLU(0.1, upper=0.4)'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easily hackable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made sure that skorch is as hackable as possible. On the neural net classes, look out for methods that start with `get_`, such as `get_loss`, or override the `train_step` itself. On the callbacks, look for methods that start with `on_`, such as `on_train_begin`. They always receive the associated `net` instance as the first parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_tweet(msg):\n",
    "    print(\"*tweet* {}\".format(msg))\n",
    "\n",
    "\n",
    "class TweetAccuracy(Callback):\n",
    "    def __init__(self, min_accuracy=0.99):\n",
    "        self.min_accuracy = min_accuracy\n",
    "\n",
    "    def on_train_end(self, net, **kwargs):\n",
    "        best_accuracy = max(net.history[:, 'valid_acc'])\n",
    "        if best_accuracy >= self.min_accuracy:\n",
    "            msg = \"Reached an accuracy of {:.4f}!!!\".format(best_accuracy)\n",
    "            send_tweet(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradAccNet(NeuralNetClassifier):\n",
    "    def __init__(self, *args, acc_steps=2, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.acc_steps = acc_steps\n",
    "\n",
    "    def get_loss(self, *args, **kwargs):\n",
    "        loss = super().get_loss(*args, **kwargs)\n",
    "        return loss / self.acc_steps  # normalize loss\n",
    "\n",
    "    def train_step(self, Xi, yi, **fit_params):\n",
    "        n_train_batches = len(self.history[-1, 'batches'])\n",
    "        step = self.train_step_single(Xi, yi, **fit_params)\n",
    "\n",
    "        if n_train_batches % self.acc_steps == 0:\n",
    "            self.optimizer_.step()\n",
    "            self.optimizer_.zero_grad()\n",
    "        return step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_acc_net = GradAccNet(MyModule, callbacks=[TweetAccuracy(min_accuracy=0.7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.3835\u001b[0m       \u001b[32m0.5007\u001b[0m        \u001b[35m0.3582\u001b[0m  0.1253\n",
      "      2        \u001b[36m0.3625\u001b[0m       \u001b[32m0.5172\u001b[0m        \u001b[35m0.3446\u001b[0m  0.1314\n",
      "      3        \u001b[36m0.3501\u001b[0m       \u001b[32m0.5622\u001b[0m        \u001b[35m0.3361\u001b[0m  0.1157\n",
      "      4        \u001b[36m0.3399\u001b[0m       \u001b[32m0.6052\u001b[0m        \u001b[35m0.3296\u001b[0m  0.1082\n",
      "      5        \u001b[36m0.3352\u001b[0m       \u001b[32m0.6312\u001b[0m        \u001b[35m0.3240\u001b[0m  0.1283\n",
      "      6        \u001b[36m0.3287\u001b[0m       \u001b[32m0.6637\u001b[0m        \u001b[35m0.3184\u001b[0m  0.1125\n",
      "      7        \u001b[36m0.3256\u001b[0m       \u001b[32m0.6932\u001b[0m        \u001b[35m0.3133\u001b[0m  0.1257\n",
      "      8        \u001b[36m0.3209\u001b[0m       \u001b[32m0.7041\u001b[0m        \u001b[35m0.3082\u001b[0m  0.1100\n",
      "      9        \u001b[36m0.3149\u001b[0m       \u001b[32m0.7186\u001b[0m        \u001b[35m0.3028\u001b[0m  0.1106\n",
      "     10        \u001b[36m0.3113\u001b[0m       \u001b[32m0.7286\u001b[0m        \u001b[35m0.2978\u001b[0m  0.1108\n",
      "*tweet* Reached an accuracy of 0.7286!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class '__main__.GradAccNet'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "    (output): Linear(in_features=10, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_acc_net.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
