{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcase some of the features of skorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces you to some of the nice features offered by [skorch](https://github.com/skorch-dev/skorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a companion notebook the PyCon/PyData Berlin 2019 presentation that can be found [here](https://github.com/BenjaminBossan/public-presentations/blob/master/20191010-pycon-pydata/presentation.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeds and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'  # choose 'cuda' or 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A toy binary classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(10000, 20, n_informative=10, random_state=0)\n",
    "X = X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 20), (10000,), 0.5003)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape, y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the PyTorch `module`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a vanilla neural network with one hidden layer. The output layer should have 2 output units since there are two classes. In addition, it should have a softmax nonlinearity, because later, when calling `predict_proba`, the output from the `forward` call will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_units=10, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(20, num_units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output = nn.Linear(num_units, 2)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = F.relu(self.dense(X))\n",
    "        X = self.dropout(X)\n",
    "        X = F.softmax(self.output(X), dim=-1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction of boilerplate code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure PyTorch implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show a basic training loop implemented with just PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "loader_train = DataLoader(ds_train, batch_size=256, shuffle=True)\n",
    "ds_valid = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))\n",
    "loader_valid = DataLoader(ds_valid, batch_size=256)\n",
    "module = MyModule(num_units=50).to(DEVICE)\n",
    "optimizer = torch.optim.SGD(module.parameters(), lr=0.02)\n",
    "criterion = nn.NLLLoss()\n",
    "template = \"epoch: {} | loss train: {:.4f} | loss valid: {:.4f} | acc valid: {:.4f} | dur: {:.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | loss train: 0.7184 | loss valid: 0.6442 | acc valid: 0.6205 | dur: 0.124\n",
      "epoch: 2 | loss train: 0.6249 | loss valid: 0.6044 | acc valid: 0.6877 | dur: 0.138\n",
      "epoch: 3 | loss train: 0.5889 | loss valid: 0.5691 | acc valid: 0.7178 | dur: 0.127\n",
      "epoch: 4 | loss train: 0.5610 | loss valid: 0.5460 | acc valid: 0.7309 | dur: 0.117\n",
      "epoch: 5 | loss train: 0.5437 | loss valid: 0.5190 | acc valid: 0.7558 | dur: 0.110\n",
      "epoch: 6 | loss train: 0.5186 | loss valid: 0.5070 | acc valid: 0.7565 | dur: 0.120\n",
      "epoch: 7 | loss train: 0.5023 | loss valid: 0.4884 | acc valid: 0.7801 | dur: 0.121\n",
      "epoch: 8 | loss train: 0.4923 | loss valid: 0.4761 | acc valid: 0.7767 | dur: 0.122\n",
      "epoch: 9 | loss train: 0.4779 | loss valid: 0.4555 | acc valid: 0.7919 | dur: 0.118\n",
      "epoch: 10 | loss train: 0.4776 | loss valid: 0.4525 | acc valid: 0.7893 | dur: 0.116\n",
      "epoch: 11 | loss train: 0.4591 | loss valid: 0.4543 | acc valid: 0.7902 | dur: 0.110\n",
      "epoch: 12 | loss train: 0.4423 | loss valid: 0.4296 | acc valid: 0.8195 | dur: 0.111\n",
      "epoch: 13 | loss train: 0.4438 | loss valid: 0.4326 | acc valid: 0.8035 | dur: 0.133\n",
      "epoch: 14 | loss train: 0.4308 | loss valid: 0.4255 | acc valid: 0.8068 | dur: 0.111\n",
      "epoch: 15 | loss train: 0.4193 | loss valid: 0.4091 | acc valid: 0.8184 | dur: 0.104\n",
      "epoch: 16 | loss train: 0.4171 | loss valid: 0.4117 | acc valid: 0.8190 | dur: 0.114\n",
      "epoch: 17 | loss train: 0.4098 | loss valid: 0.4075 | acc valid: 0.8150 | dur: 0.108\n",
      "epoch: 18 | loss train: 0.4066 | loss valid: 0.3956 | acc valid: 0.8192 | dur: 0.122\n",
      "epoch: 19 | loss train: 0.4033 | loss valid: 0.3938 | acc valid: 0.8285 | dur: 0.120\n",
      "epoch: 20 | loss train: 0.3891 | loss valid: 0.3853 | acc valid: 0.8300 | dur: 0.114\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    tic = time.time()\n",
    "    losses_train = []\n",
    "    for Xb, yb in loader_train:\n",
    "        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "        y_proba = module(Xb)\n",
    "        loss = criterion(torch.log(y_proba), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses_train.append(loss.item())\n",
    "        \n",
    "    losses_valid = []\n",
    "    accuracy_valid = []\n",
    "    for Xb, yb in loader_valid:\n",
    "        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "        y_proba = module(Xb)\n",
    "        loss = criterion(torch.log(y_proba), yb)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses_valid.append(loss.item())\n",
    "        accuracy_valid.append(accuracy_score(yb.cpu().numpy(), y_proba.argmax(1).cpu().numpy()))\n",
    "        \n",
    "    toc = time.time() - tic\n",
    "    print(template.format(\n",
    "        epoch + 1, np.mean(losses_train), np.mean(losses_valid), np.mean(accuracy_valid), toc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with skorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show how to achieve the same outcome with skorch. Note how we don't need to make any adjustments to the `module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    MyModule,\n",
    "    module__num_units=50,\n",
    "    max_epochs=20,\n",
    "    lr=0.02,\n",
    "    batch_size=256,\n",
    "    iterator_train__shuffle=True,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7102\u001b[0m       \u001b[32m0.7066\u001b[0m        \u001b[35m0.6139\u001b[0m  0.1345\n",
      "      2        \u001b[36m0.6294\u001b[0m       \u001b[32m0.7446\u001b[0m        \u001b[35m0.5722\u001b[0m  0.1276\n",
      "      3        \u001b[36m0.5893\u001b[0m       \u001b[32m0.7656\u001b[0m        \u001b[35m0.5414\u001b[0m  0.1256\n",
      "      4        \u001b[36m0.5662\u001b[0m       \u001b[32m0.7746\u001b[0m        \u001b[35m0.5166\u001b[0m  0.1284\n",
      "      5        \u001b[36m0.5473\u001b[0m       \u001b[32m0.7921\u001b[0m        \u001b[35m0.4957\u001b[0m  0.1253\n",
      "      6        \u001b[36m0.5242\u001b[0m       \u001b[32m0.8036\u001b[0m        \u001b[35m0.4752\u001b[0m  0.1270\n",
      "      7        \u001b[36m0.5048\u001b[0m       \u001b[32m0.8121\u001b[0m        \u001b[35m0.4569\u001b[0m  0.1282\n",
      "      8        \u001b[36m0.4934\u001b[0m       \u001b[32m0.8186\u001b[0m        \u001b[35m0.4410\u001b[0m  0.1349\n",
      "      9        \u001b[36m0.4894\u001b[0m       \u001b[32m0.8266\u001b[0m        \u001b[35m0.4268\u001b[0m  0.1368\n",
      "     10        \u001b[36m0.4623\u001b[0m       \u001b[32m0.8311\u001b[0m        \u001b[35m0.4141\u001b[0m  0.1339\n",
      "     11        \u001b[36m0.4557\u001b[0m       \u001b[32m0.8391\u001b[0m        \u001b[35m0.4010\u001b[0m  0.1337\n",
      "     12        \u001b[36m0.4469\u001b[0m       \u001b[32m0.8481\u001b[0m        \u001b[35m0.3896\u001b[0m  0.1733\n",
      "     13        \u001b[36m0.4374\u001b[0m       \u001b[32m0.8541\u001b[0m        \u001b[35m0.3792\u001b[0m  0.1491\n",
      "     14        \u001b[36m0.4290\u001b[0m       \u001b[32m0.8611\u001b[0m        \u001b[35m0.3706\u001b[0m  0.1213\n",
      "     15        \u001b[36m0.4194\u001b[0m       \u001b[32m0.8661\u001b[0m        \u001b[35m0.3618\u001b[0m  0.1188\n",
      "     16        0.4211       \u001b[32m0.8691\u001b[0m        \u001b[35m0.3541\u001b[0m  0.1282\n",
      "     17        \u001b[36m0.4097\u001b[0m       \u001b[32m0.8741\u001b[0m        \u001b[35m0.3465\u001b[0m  0.1433\n",
      "     18        \u001b[36m0.3936\u001b[0m       \u001b[32m0.8746\u001b[0m        \u001b[35m0.3388\u001b[0m  0.2403\n",
      "     19        \u001b[36m0.3882\u001b[0m       \u001b[32m0.8791\u001b[0m        \u001b[35m0.3321\u001b[0m  0.4014\n",
      "     20        \u001b[36m0.3837\u001b[0m       0.8771        \u001b[35m0.3275\u001b[0m  0.3335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compatibility with sklearn API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor sklearn metrics during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EpochScoring\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = EpochScoring(\n",
    "    scoring=roc_auc_score,  # <-- just passing 'roc_auc' would also work\n",
    "    lower_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    MyModule,\n",
    "    module__num_units=50,\n",
    "    max_epochs=20,\n",
    "    lr=0.02,\n",
    "    batch_size=256,\n",
    "    iterator_train__shuffle=True,\n",
    "    device=DEVICE,\n",
    "    callbacks=[auc],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    roc_auc_score    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ---------------  ------------  -----------  ------------  ------\n",
      "      1           \u001b[36m0.6882\u001b[0m        \u001b[32m0.6891\u001b[0m       \u001b[35m0.6882\u001b[0m        \u001b[31m0.6065\u001b[0m  0.3627\n",
      "      2           \u001b[36m0.7326\u001b[0m        \u001b[32m0.6141\u001b[0m       \u001b[35m0.7326\u001b[0m        \u001b[31m0.5624\u001b[0m  0.5033\n",
      "      3           \u001b[36m0.7586\u001b[0m        \u001b[32m0.5755\u001b[0m       \u001b[35m0.7586\u001b[0m        \u001b[31m0.5306\u001b[0m  0.4027\n",
      "      4           \u001b[36m0.7786\u001b[0m        \u001b[32m0.5533\u001b[0m       \u001b[35m0.7786\u001b[0m        \u001b[31m0.5038\u001b[0m  0.4762\n",
      "      5           \u001b[36m0.7926\u001b[0m        \u001b[32m0.5285\u001b[0m       \u001b[35m0.7926\u001b[0m        \u001b[31m0.4809\u001b[0m  0.4843\n",
      "      6           \u001b[36m0.8086\u001b[0m        \u001b[32m0.5108\u001b[0m       \u001b[35m0.8086\u001b[0m        \u001b[31m0.4603\u001b[0m  0.4090\n",
      "      7           \u001b[36m0.8226\u001b[0m        \u001b[32m0.4872\u001b[0m       \u001b[35m0.8226\u001b[0m        \u001b[31m0.4411\u001b[0m  0.3477\n",
      "      8           \u001b[36m0.8311\u001b[0m        \u001b[32m0.4737\u001b[0m       \u001b[35m0.8311\u001b[0m        \u001b[31m0.4231\u001b[0m  0.4181\n",
      "      9           \u001b[36m0.8406\u001b[0m        \u001b[32m0.4631\u001b[0m       \u001b[35m0.8406\u001b[0m        \u001b[31m0.4093\u001b[0m  0.6315\n",
      "     10           \u001b[36m0.8481\u001b[0m        \u001b[32m0.4517\u001b[0m       \u001b[35m0.8481\u001b[0m        \u001b[31m0.3953\u001b[0m  0.2951\n",
      "     11           \u001b[36m0.8571\u001b[0m        \u001b[32m0.4430\u001b[0m       \u001b[35m0.8571\u001b[0m        \u001b[31m0.3839\u001b[0m  0.3842\n",
      "     12           \u001b[36m0.8606\u001b[0m        \u001b[32m0.4344\u001b[0m       \u001b[35m0.8606\u001b[0m        \u001b[31m0.3728\u001b[0m  0.3916\n",
      "     13           \u001b[36m0.8711\u001b[0m        \u001b[32m0.4232\u001b[0m       \u001b[35m0.8711\u001b[0m        \u001b[31m0.3612\u001b[0m  0.6126\n",
      "     14           \u001b[36m0.8756\u001b[0m        \u001b[32m0.4164\u001b[0m       \u001b[35m0.8756\u001b[0m        \u001b[31m0.3516\u001b[0m  0.5675\n",
      "     15           \u001b[36m0.8776\u001b[0m        \u001b[32m0.4147\u001b[0m       \u001b[35m0.8776\u001b[0m        \u001b[31m0.3440\u001b[0m  0.7206\n",
      "     16           0.8761        \u001b[32m0.3941\u001b[0m       0.8761        \u001b[31m0.3365\u001b[0m  0.6542\n",
      "     17           \u001b[36m0.8786\u001b[0m        0.3942       \u001b[35m0.8786\u001b[0m        \u001b[31m0.3288\u001b[0m  0.5354\n",
      "     18           \u001b[36m0.8835\u001b[0m        \u001b[32m0.3895\u001b[0m       \u001b[35m0.8836\u001b[0m        \u001b[31m0.3225\u001b[0m  0.4949\n",
      "     19           \u001b[36m0.8851\u001b[0m        \u001b[32m0.3860\u001b[0m       \u001b[35m0.8851\u001b[0m        \u001b[31m0.3155\u001b[0m  0.5542\n",
      "     20           \u001b[36m0.8905\u001b[0m        \u001b[32m0.3814\u001b[0m       \u001b[35m0.8906\u001b[0m        \u001b[31m0.3093\u001b[0m  0.4685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support for the basic methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = net.predict(X[:5])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54133016, 0.4586698 ],\n",
       "       [0.13347742, 0.86652255],\n",
       "       [0.2590016 , 0.7409983 ],\n",
       "       [0.47930065, 0.5206993 ],\n",
       "       [0.32075438, 0.6792456 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = net.predict_proba(X[:5])\n",
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     21           0.8905        \u001b[32m0.3754\u001b[0m       0.8906        \u001b[31m0.3052\u001b[0m  0.7788\n",
      "     22           \u001b[36m0.8955\u001b[0m        \u001b[32m0.3678\u001b[0m       \u001b[35m0.8956\u001b[0m        \u001b[31m0.2995\u001b[0m  0.3332\n",
      "     23           \u001b[36m0.8995\u001b[0m        \u001b[32m0.3596\u001b[0m       \u001b[35m0.8996\u001b[0m        \u001b[31m0.2936\u001b[0m  0.4569\n",
      "     24           \u001b[36m0.9025\u001b[0m        0.3622       \u001b[35m0.9025\u001b[0m        \u001b[31m0.2874\u001b[0m  0.5510\n",
      "     25           \u001b[36m0.9075\u001b[0m        \u001b[32m0.3546\u001b[0m       \u001b[35m0.9075\u001b[0m        \u001b[31m0.2829\u001b[0m  0.3548\n",
      "     26           \u001b[36m0.9080\u001b[0m        \u001b[32m0.3536\u001b[0m       \u001b[35m0.9080\u001b[0m        \u001b[31m0.2788\u001b[0m  0.4301\n",
      "     27           0.9070        0.3545       0.9070        \u001b[31m0.2753\u001b[0m  0.2874\n",
      "     28           \u001b[36m0.9080\u001b[0m        \u001b[32m0.3441\u001b[0m       0.9080        \u001b[31m0.2715\u001b[0m  0.2989\n",
      "     29           \u001b[36m0.9115\u001b[0m        \u001b[32m0.3428\u001b[0m       \u001b[35m0.9115\u001b[0m        \u001b[31m0.2670\u001b[0m  0.1904\n",
      "     30           \u001b[36m0.9120\u001b[0m        \u001b[32m0.3359\u001b[0m       \u001b[35m0.9120\u001b[0m        \u001b[31m0.2630\u001b[0m  0.1878\n",
      "     31           0.9095        \u001b[32m0.3344\u001b[0m       0.9095        \u001b[31m0.2598\u001b[0m  0.2340\n",
      "     32           0.9090        \u001b[32m0.3270\u001b[0m       0.9090        \u001b[31m0.2568\u001b[0m  0.1931\n",
      "     33           0.9110        \u001b[32m0.3263\u001b[0m       0.9110        \u001b[31m0.2538\u001b[0m  0.3510\n",
      "     34           \u001b[36m0.9140\u001b[0m        \u001b[32m0.3191\u001b[0m       \u001b[35m0.9140\u001b[0m        \u001b[31m0.2503\u001b[0m  0.2081\n",
      "     35           \u001b[36m0.9140\u001b[0m        0.3268       0.9140        \u001b[31m0.2482\u001b[0m  0.3725\n",
      "     36           \u001b[36m0.9165\u001b[0m        \u001b[32m0.3154\u001b[0m       \u001b[35m0.9165\u001b[0m        \u001b[31m0.2442\u001b[0m  0.7780\n",
      "     37           0.9150        0.3178       0.9150        \u001b[31m0.2428\u001b[0m  0.3554\n",
      "     38           0.9150        0.3157       0.9150        \u001b[31m0.2412\u001b[0m  0.3000\n",
      "     39           \u001b[36m0.9170\u001b[0m        \u001b[32m0.3120\u001b[0m       \u001b[35m0.9170\u001b[0m        \u001b[31m0.2379\u001b[0m  0.3732\n",
      "     40           \u001b[36m0.9220\u001b[0m        \u001b[32m0.3101\u001b[0m       \u001b[35m0.9220\u001b[0m        \u001b[31m0.2352\u001b[0m  0.4243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.partial_fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_params();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.set_params(verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = clone(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([5.74459791, 6.21544385, 6.15612483]),\n",
       " 'score_time': array([0.13037705, 0.07822895, 0.05389428]),\n",
       " 'test_score': array([0.87762448, 0.86082783, 0.8817527 ])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(net, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use inside an sklearn `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scale',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('net',\n",
       "                 <class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       "))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('net', net),\n",
    "])\n",
    "\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58262163, 0.4173784 ],\n",
       "       [0.29107776, 0.7089222 ],\n",
       "       [0.2788414 , 0.7211586 ],\n",
       "       [0.51545745, 0.48454258],\n",
       "       [0.37120497, 0.628795  ]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict_proba(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the whole pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the whole pipeline, including preprocessing and the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bbossan/anaconda3/envs/skorch/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type MyModule. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "with open('my_pipeline.pickle', 'wb') as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No special adjustments need to be made to perform a hyperparameter search on the net parameters. We can even search on the `__init__` parameters of our `module` by using the `'module__'` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_epochs': [10, 20],\n",
    "    'optimizer__momentum': [0.0, 0.9],\n",
    "    'module__num_units': [10, 50],  # <-- just works\n",
    "    'module__dropout': [0, 0.5],  # <-- just works\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.0, total=   3.2s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.0, total=   3.6s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.0, total=   2.4s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.9, total=   2.8s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.9, total=   5.4s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=10, optimizer__momentum=0.9, total=   3.7s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.0, total=   7.1s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.0, total=   4.7s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.0, total=   3.4s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.9, total=   3.5s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.9, total=   3.7s\n",
      "[CV] max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0, module__num_units=50, optimizer__momentum=0.9, total=   3.4s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0, total=   3.3s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0, total=   3.8s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0, total=   3.7s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9, total=   3.2s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9, total=   3.3s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9, total=   2.9s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0, total=   3.9s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0, total=   7.8s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0, total=   6.5s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9, total=   2.6s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9, total=   4.6s\n",
      "[CV] max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=10, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9, total=   4.1s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.0, total=   5.1s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.0, total=   8.5s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.0, total=   6.7s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.9, total=   8.7s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.9, total=   9.4s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=10, optimizer__momentum=0.9, total=   8.5s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.0, total=   9.5s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.0, total=   8.9s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.0, total=   6.4s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.9, total=   8.3s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.9, total=   6.8s\n",
      "[CV] max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0, module__num_units=50, optimizer__momentum=0.9, total=   5.6s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0, total=   8.2s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0, total=   8.4s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.0, total=   6.7s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9, total=   8.1s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9, total=   7.6s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=10, optimizer__momentum=0.9, total=   6.5s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0, total=   7.2s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0, total=   7.1s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.0, total=   6.5s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9, total=   6.3s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9, total=   7.1s\n",
      "[CV] max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9 \n",
      "[CV]  max_epochs=20, module__dropout=0.5, module__num_units=50, optimizer__momentum=0.9, total=   6.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29min 19s, sys: 17.6 s, total: 29min 37s\n",
      "Wall time: 4min 46s\n"
     ]
    }
   ],
   "source": [
    "%time search = GridSearchCV(net, params, verbose=2, cv=3).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.956,\n",
       " {'max_epochs': 20,\n",
       "  'module__dropout': 0,\n",
       "  'module__num_units': 50,\n",
       "  'optimizer__momentum': 0.9})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_, search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can grid search the parameters of almost everything:\n",
    "\n",
    "- NeuralNet\n",
    "- module\n",
    "- optimizer\n",
    "- criterion\n",
    "- DataLoader\n",
    "- callbacks\n",
    "\n",
    "Just use the `__` notation known from sklearn, e.g. `optimizer__momentum` to set the `momentum` parameter of the optimizer. To make a search on callback parameters, give the parameter a name by passing a tuple of name and callback (like in an sklearn `Pipeline`). skorch uses the name, e.g. `'mycb'`, to dispatch to the callback. E.g.:\n",
    "\n",
    "```\n",
    "net = NeuralNetClassifier(..., callbacks=[('mycb', MyCallback(foo=1))])\n",
    "params = {'callbacks__mycb__foo': [1, 2, 3]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swap skorch net for any other sklearn estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since skorch's estimators work like any other sklearn estimator, you can swap them out to see which one leads to the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare our neural network with a logistic regression and a KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.set_params(**search.best_params_)  # use the best parameters from grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('model', net),\n",
    "])\n",
    "params = {'model': [net, LogisticRegression(), KNeighborsClassifier()]}\n",
    "search = GridSearchCV(pipe, params, verbose=2, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] model=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
      "  module_=MyModule(\n",
      "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
      "  ),\n",
      ") \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  model=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
      "  module_=MyModule(\n",
      "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
      "  ),\n",
      "), total=   8.2s\n",
      "[CV] model=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
      "  module_=MyModule(\n",
      "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
      "  ),\n",
      ") \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  model=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
      "  module_=MyModule(\n",
      "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
      "  ),\n",
      "), total=   6.4s\n",
      "[CV] model=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
      "  module_=MyModule(\n",
      "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
      "  ),\n",
      ") \n",
      "[CV]  model=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
      "  module_=MyModule(\n",
      "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
      "  ),\n",
      "), total=   5.4s\n",
      "[CV] model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False) \n",
      "[CV]  model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), total=   0.0s\n",
      "[CV] model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False) \n",
      "[CV]  model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), total=   0.0s\n",
      "[CV] model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False) \n",
      "[CV]  model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), total=   0.0s\n",
      "[CV] model=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform') \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bbossan/anaconda3/envs/skorch/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/bbossan/anaconda3/envs/skorch/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/bbossan/anaconda3/envs/skorch/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  model=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform'), total=   0.8s\n",
      "[CV] model=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform') \n",
      "[CV]  model=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform'), total=   0.8s\n",
      "[CV] model=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform') \n",
      "[CV]  model=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform'), total=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   22.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 12s, sys: 4.01 s, total: 5min 16s\n",
      "Wall time: 32.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('scale',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('model',\n",
       "                                        <class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (output): Linear(in_...\n",
       "                                                      max_iter=100,\n",
       "                                                      multi_class='warn',\n",
       "                                                      n_jobs=None, penalty='l2',\n",
       "                                                      random_state=None,\n",
       "                                                      solver='warn', tol=0.0001,\n",
       "                                                      verbose=0,\n",
       "                                                      warm_start=False),\n",
       "                                   KNeighborsClassifier(algorithm='auto',\n",
       "                                                        leaf_size=30,\n",
       "                                                        metric='minkowski',\n",
       "                                                        metric_params=None,\n",
       "                                                        n_jobs=None,\n",
       "                                                        n_neighbors=5, p=2,\n",
       "                                                        weights='uniform')]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9497,\n",
       " {'model': <class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "    module_=MyModule(\n",
       "      (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "      (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "    ),\n",
       "  )})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_, search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distributed `GridSearchCV` with dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a distributed hyperparameter search, you need `dask` and `dask.distributed`:\n",
    "\n",
    "`$ pip install dask distributed`\n",
    "\n",
    "Setup your dask workers as described [here](https://docs.dask.org/en/latest/setup.html).\n",
    "\n",
    "Then run the following lines:\n",
    "\n",
    "```\n",
    "from dask.distributed import Client\n",
    "from joblib import parallel_backend\n",
    "\n",
    "client = Client('127.0.0.1:8786')\n",
    "\n",
    "search = GridSearchCV(net, params, verbose=2, cv=3)\n",
    "\n",
    "with parallel_backend('dask'):\n",
    "    search.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More additions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the `state_dict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to save the `state_dict` of our module (and maybe our optimizer), we can either use the `Checkpoint` callback or call the `save_params` method. Use `load_params` to load the `state_dict` later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = Checkpoint(monitor='valid_loss_best', dirname='exp1')\n",
    "net = NeuralNetClassifier(\n",
    "    MyModule,\n",
    "    module__num_units=50,\n",
    "    max_epochs=20,\n",
    "    lr=0.02,\n",
    "    batch_size=256,\n",
    "    iterator_train__shuffle=True,\n",
    "    device=DEVICE,\n",
    "    callbacks=[cp],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m0.7194\u001b[0m       \u001b[32m0.6932\u001b[0m        \u001b[35m0.6178\u001b[0m     +  0.4781\n",
      "      2        \u001b[36m0.6290\u001b[0m       \u001b[32m0.7341\u001b[0m        \u001b[35m0.5726\u001b[0m     +  0.3285\n",
      "      3        \u001b[36m0.5901\u001b[0m       \u001b[32m0.7606\u001b[0m        \u001b[35m0.5401\u001b[0m     +  0.4104\n",
      "      4        \u001b[36m0.5637\u001b[0m       \u001b[32m0.7716\u001b[0m        \u001b[35m0.5159\u001b[0m     +  1.2257\n",
      "      5        \u001b[36m0.5448\u001b[0m       \u001b[32m0.7931\u001b[0m        \u001b[35m0.4921\u001b[0m     +  0.6553\n",
      "      6        \u001b[36m0.5252\u001b[0m       \u001b[32m0.8061\u001b[0m        \u001b[35m0.4709\u001b[0m     +  0.7063\n",
      "      7        \u001b[36m0.5090\u001b[0m       \u001b[32m0.8156\u001b[0m        \u001b[35m0.4534\u001b[0m     +  0.6670\n",
      "      8        \u001b[36m0.5042\u001b[0m       \u001b[32m0.8261\u001b[0m        \u001b[35m0.4392\u001b[0m     +  0.7381\n",
      "      9        \u001b[36m0.4866\u001b[0m       \u001b[32m0.8321\u001b[0m        \u001b[35m0.4250\u001b[0m     +  0.5423\n",
      "     10        \u001b[36m0.4735\u001b[0m       \u001b[32m0.8421\u001b[0m        \u001b[35m0.4128\u001b[0m     +  0.4542\n",
      "     11        \u001b[36m0.4635\u001b[0m       \u001b[32m0.8486\u001b[0m        \u001b[35m0.4010\u001b[0m     +  0.4460\n",
      "     12        \u001b[36m0.4596\u001b[0m       \u001b[32m0.8551\u001b[0m        \u001b[35m0.3927\u001b[0m     +  0.4214\n",
      "     13        \u001b[36m0.4488\u001b[0m       \u001b[32m0.8601\u001b[0m        \u001b[35m0.3828\u001b[0m     +  0.2816\n",
      "     14        \u001b[36m0.4386\u001b[0m       \u001b[32m0.8646\u001b[0m        \u001b[35m0.3742\u001b[0m     +  0.5954\n",
      "     15        \u001b[36m0.4340\u001b[0m       \u001b[32m0.8666\u001b[0m        \u001b[35m0.3653\u001b[0m     +  0.2695\n",
      "     16        \u001b[36m0.4247\u001b[0m       \u001b[32m0.8711\u001b[0m        \u001b[35m0.3580\u001b[0m     +  0.3439\n",
      "     17        \u001b[36m0.4147\u001b[0m       \u001b[32m0.8766\u001b[0m        \u001b[35m0.3502\u001b[0m     +  0.4671\n",
      "     18        \u001b[36m0.4111\u001b[0m       \u001b[32m0.8791\u001b[0m        \u001b[35m0.3448\u001b[0m     +  0.4282\n",
      "     19        \u001b[36m0.4071\u001b[0m       \u001b[32m0.8846\u001b[0m        \u001b[35m0.3380\u001b[0m     +  0.3011\n",
      "     20        \u001b[36m0.4014\u001b[0m       0.8846        \u001b[35m0.3323\u001b[0m     +  0.2073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (output): Linear(in_features=50, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, y)  # Checkpoint saves each time valid lost improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_params(\n",
    "    f_params='exp1/mynet.pt',  # <- state dict of module\n",
    "    f_optimizer='exp1/myoptimizer.pt',  # <- state dict of optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling of different data formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, skorch handles the most common data formats, even more complex ones like dictionaries. If this doesn't fit your need, just define your own `Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- numpy arrays\n",
    "- PyTorch Datasets (most)\n",
    "- dict or list of arrays\n",
    "- pandas DataFrames\n",
    "- scipy sparse CSR matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skorch comes packaged with a few useful callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import GradientNormClipping\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.callbacks import EpochScoring, BatchScoring\n",
    "from skorch.callbacks import Checkpoint, TrainEndCheckpoint, LoadInitState\n",
    "from skorch.callbacks import Freezer\n",
    "from skorch.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of skorch and Google's fire library, it is exceedingly easy to transform your training script into a nice CLI. This is what skorch and fire will automatically take care of:\n",
    "\n",
    "* help for the CLI usage\n",
    "* show docstrings in CLI help\n",
    "* set __all__ possible parameters from the command line without any manuel argument parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install fire and numpydoc:\n",
    "    \n",
    "`$ pip install fire numpydoc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It requires only a few lines of code to turn your script into a nice CLI:\n",
    "\n",
    "```\n",
    "def main(..., **kwargs):\n",
    "    model = ...  # put model definition here\n",
    "\n",
    "    model = parse_args(kwargs)(model)  # <-- add this line\n",
    "    \n",
    "    model.fit(X, y)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(main)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the complete train.py script. Note the few lines that needed to be added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Simple training script for a MLP classifier.\r\n",
      "\r\n",
      "See accompanying `pycon_showcase_skorch.ipynb` for details.\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "import pickle\r\n",
      "\r\n",
      "import fire\r\n",
      "import numpy as np\r\n",
      "from sklearn.datasets import make_classification\r\n",
      "from skorch import NeuralNetClassifier\r\n",
      "import torch\r\n",
      "from torch import nn\r\n",
      "\r\n",
      "from skorch.helper import parse_args\r\n",
      "\r\n",
      "\r\n",
      "np.random.seed(0)\r\n",
      "torch.manual_seed(0)\r\n",
      "torch.cuda.manual_seed(0)\r\n",
      "\r\n",
      "\r\n",
      "# number of input features\r\n",
      "N_FEATURES = 20\r\n",
      "\r\n",
      "# number of classes\r\n",
      "N_CLASSES = 2\r\n",
      "\r\n",
      "# custom defaults for net\r\n",
      "DEFAULTS = {\r\n",
      "    'batch_size': 256,\r\n",
      "    'module__hidden_units': 30,\r\n",
      "}\r\n",
      "\r\n",
      "\r\n",
      "class MLPClassifier(nn.Module):\r\n",
      "    \"\"\"A simple multi-layer perceptron module.\r\n",
      "\r\n",
      "    This can be adapted for usage in different contexts, e.g. binary\r\n",
      "    and multi-class classification, regression, etc.\r\n",
      "\r\n",
      "    Note: This docstring is used to create the help for the CLI.\r\n",
      "\r\n",
      "    Parameters\r\n",
      "    ----------\r\n",
      "    hidden_units : int (default=10)\r\n",
      "      Number of units in hidden layers.\r\n",
      "\r\n",
      "    num_hidden : int (default=1)\r\n",
      "      Number of hidden layers.\r\n",
      "\r\n",
      "    nonlin : torch.nn.Module instance (default=torch.nn.ReLU())\r\n",
      "      Non-linearity to apply after hidden layers.\r\n",
      "\r\n",
      "    dropout : float (default=0)\r\n",
      "      Dropout rate. Dropout is applied between layers.\r\n",
      "\r\n",
      "    \"\"\"\r\n",
      "    def __init__(\r\n",
      "            self,\r\n",
      "            hidden_units=10,\r\n",
      "            num_hidden=1,\r\n",
      "            nonlin=nn.ReLU(),\r\n",
      "            dropout=0,\r\n",
      "    ):\r\n",
      "        super().__init__()\r\n",
      "        self.hidden_units = hidden_units\r\n",
      "        self.num_hidden = num_hidden\r\n",
      "        self.nonlin = nonlin\r\n",
      "        self.dropout = dropout\r\n",
      "\r\n",
      "        self.reset_params()\r\n",
      "\r\n",
      "    def reset_params(self):\r\n",
      "        \"\"\"(Re)set all parameters.\"\"\"\r\n",
      "        units = [N_FEATURES]\r\n",
      "        units += [self.hidden_units] * self.num_hidden\r\n",
      "        units += [N_CLASSES]\r\n",
      "\r\n",
      "        sequence = []\r\n",
      "        for u0, u1 in zip(units, units[1:]):\r\n",
      "            sequence.append(nn.Linear(u0, u1))\r\n",
      "            sequence.append(self.nonlin)\r\n",
      "            sequence.append(nn.Dropout(self.dropout))\r\n",
      "\r\n",
      "        sequence = sequence[:-2]\r\n",
      "        self.sequential = nn.Sequential(*sequence)\r\n",
      "\r\n",
      "    def forward(self, X):\r\n",
      "        return nn.Softmax(dim=-1)(self.sequential(X))\r\n",
      "\r\n",
      "\r\n",
      "def get_data(n_samples=100):\r\n",
      "    \"\"\"Get synthetic classification data with n_samples samples.\"\"\"\r\n",
      "    X, y = make_classification(\r\n",
      "        n_samples=n_samples,\r\n",
      "        n_features=N_FEATURES,\r\n",
      "        n_classes=N_CLASSES,\r\n",
      "        random_state=0,\r\n",
      "    )\r\n",
      "    X = X.astype(np.float32)\r\n",
      "    return X, y\r\n",
      "\r\n",
      "\r\n",
      "def save_model(model, output_file):\r\n",
      "    \"\"\"Save model to output_file, if given\"\"\"\r\n",
      "    if not output_file:\r\n",
      "        return\r\n",
      "\r\n",
      "    with open(output_file, 'wb') as f:\r\n",
      "        pickle.dump(model, f)\r\n",
      "    print(\"Saved model to file '{}'.\".format(output_file))\r\n",
      "\r\n",
      "\r\n",
      "def main(n_samples=100, output_file=None, **kwargs):\r\n",
      "    \"\"\"Train an MLP classifier on synthetic data.\r\n",
      "\r\n",
      "    n_samples : int (default=100)\r\n",
      "      Number of training samples\r\n",
      "\r\n",
      "    output_file : str (default=None)\r\n",
      "      If not None, file name used to save the model.\r\n",
      "\r\n",
      "    kwargs : dict\r\n",
      "      Additional model parameters.\r\n",
      "\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    model = NeuralNetClassifier(MLPClassifier)\r\n",
      "    # important: wrap the model with the parsed arguments\r\n",
      "    parsed = parse_args(kwargs, defaults=DEFAULTS)\r\n",
      "    model = parsed(model)\r\n",
      "\r\n",
      "    X, y = get_data(n_samples=n_samples)\r\n",
      "    print(\"Training MLP classifier\")\r\n",
      "    model.fit(X, y)\r\n",
      "\r\n",
      "    save_model(model, output_file)\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    fire.Fire(main)\r\n"
     ]
    }
   ],
   "source": [
    "!cat train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNAME\u001b[0m\r\n",
      "    train.py - Train an MLP classifier on synthetic data.\r\n",
      "\r\n",
      "\u001b[1mSYNOPSIS\u001b[0m\r\n",
      "    train.py <flags>\r\n",
      "\r\n",
      "\u001b[1mDESCRIPTION\u001b[0m\r\n",
      "    n_samples : int (default=100)\r\n",
      "      Number of training samples\r\n",
      "\r\n",
      "    output_file : str (default=None)\r\n",
      "      If not None, file name used to save the model.\r\n",
      "\r\n",
      "    kwargs : dict\r\n",
      "      Additional model parameters.\r\n",
      "\r\n",
      "\u001b[1mFLAGS\u001b[0m\r\n",
      "    --n_samples=\u001b[4mN_SAMPLES\u001b[0m\r\n",
      "    --output_file=\u001b[4mOUTPUT_FILE\u001b[0m\r\n",
      "    Additional flags are accepted.\r\n"
     ]
    }
   ],
   "source": [
    "!python train.py -- --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-specific help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the help for the model-specific parameters.\r\n",
      "To invoke help for the remaining options, run:\r\n",
      "python train.py -- --help\r\n",
      "\r\n",
      "<NeuralNetClassifier> options:\r\n",
      "  --module : torch module (class or instance)\r\n",
      "    A PyTorch :class:`~torch.nn.Module`. In general, the\r\n",
      "    uninstantiated class should be passed, although instantiated\r\n",
      "    modules will also work.\r\n",
      "  --criterion : torch criterion (class, default=torch.nn.NLLLoss)\r\n",
      "    Negative log likelihood loss. Note that the module should return\r\n",
      "    probabilities, the log is applied during ``get_loss``.\r\n",
      "  --optimizer : torch optim (class, default=torch.optim.SGD)\r\n",
      "    The uninitialized optimizer (update rule) used to optimize the\r\n",
      "    module\r\n",
      "  --lr : float (default=0.01)\r\n",
      "    Learning rate passed to the optimizer. You may use ``lr`` instead\r\n",
      "    of using ``optimizer__lr``, which would result in the same outcome.\r\n",
      "  --max_epochs : int (default=10)\r\n",
      "    The number of epochs to train for each ``fit`` call. Note that you\r\n",
      "    may keyboard-interrupt training at any time.\r\n",
      "  --batch_size : int (default=256)\r\n",
      "    Mini-batch size. Use this instead of setting\r\n",
      "    ``iterator_train__batch_size`` and ``iterator_test__batch_size``,\r\n",
      "    which would result in the same outcome. If ``batch_size`` is -1,\r\n",
      "    a single batch with all the data will be used during training\r\n",
      "    and validation.\r\n",
      "  --iterator_train : torch DataLoader\r\n",
      "    The default PyTorch :class:`~torch.utils.data.DataLoader` used for\r\n",
      "    training data.\r\n",
      "  --iterator_valid : torch DataLoader\r\n",
      "    The default PyTorch :class:`~torch.utils.data.DataLoader` used for\r\n",
      "    validation and test data, i.e. during inference.\r\n",
      "  --dataset : torch Dataset (default=skorch.dataset.Dataset)\r\n",
      "    The dataset is necessary for the incoming data to work with\r\n",
      "    pytorch's ``DataLoader``. It has to implement the ``__len__`` and\r\n",
      "    ``__getitem__`` methods. The provided dataset should be capable of\r\n",
      "    dealing with a lot of data types out of the box, so only change\r\n",
      "    this if your data is not supported. You should generally pass the\r\n",
      "    uninitialized ``Dataset`` class and define additional arguments to\r\n",
      "    X and y by prefixing them with ``dataset__``. It is also possible\r\n",
      "    to pass an initialzed ``Dataset``, in which case no additional\r\n",
      "    arguments may be passed.\r\n",
      "  --train_split : None or callable (default=skorch.dataset.CVSplit(5))\r\n",
      "    If None, there is no train/validation split. Else, train_split\r\n",
      "    should be a function or callable that is called with X and y\r\n",
      "    data and should return the tuple ``dataset_train, dataset_valid``.\r\n",
      "    The validation data may be None.\r\n",
      "  --callbacks : None or list of Callback instances (default=None)\r\n",
      "    More callbacks, in addition to those returned by\r\n",
      "    ``get_default_callbacks``. Each callback should inherit from\r\n",
      "    :class:`.Callback`. If not ``None``, a list of callbacks is\r\n",
      "    expected where the callback names are inferred from the class\r\n",
      "    name. Name conflicts are resolved by appending a count suffix\r\n",
      "    starting with 1, e.g. ``EpochScoring_1``. Alternatively,\r\n",
      "    a tuple ``(name, callback)`` can be passed, where ``name``\r\n",
      "    should be unique. Callbacks may or may not be instantiated.\r\n",
      "    The callback name can be used to set parameters on specific\r\n",
      "    callbacks (e.g., for the callback with name ``'print_log'``, use\r\n",
      "    ``net.set_params(callbacks__print_log__keys_ignored=['epoch',\r\n",
      "    'train_loss'])``).\r\n",
      "  --warm_start : bool (default=False)\r\n",
      "    Whether each fit call should lead to a re-initialization of the\r\n",
      "    module (cold start) or whether the module should be trained\r\n",
      "    further (warm start).\r\n",
      "  --verbose : int (default=1)\r\n",
      "    Control the verbosity level.\r\n",
      "  --device : str, torch.device (default='cpu')\r\n",
      "    The compute device to be used. If set to 'cuda', data in torch\r\n",
      "    tensors will be pushed to cuda tensors before being sent to the\r\n",
      "    module.\r\n",
      "\r\n",
      "<MLPClassifier> options:\r\n",
      "  --module__hidden_units : int (default=30)\r\n",
      "    Number of units in hidden layers.\r\n",
      "  --module__num_hidden : int (default=1)\r\n",
      "    Number of hidden layers.\r\n",
      "  --module__nonlin : torch.nn.Module instance (default=torch.nn.ReLU())\r\n",
      "    Non-linearity to apply after hidden layers.\r\n",
      "  --module__dropout : float (default=0)\r\n",
      "    Dropout rate. Dropout is applied between layers.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python train.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you can call the script from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP classifier\n",
      "  epoch    train_acc    train_loss    valid_loss     dur\n",
      "-------  -----------  ------------  ------------  ------\n",
      "      1       \u001b[36m0.7872\u001b[0m        \u001b[32m0.5813\u001b[0m        \u001b[35m0.5011\u001b[0m  0.0157\n",
      "      2       \u001b[36m0.9049\u001b[0m        \u001b[32m0.4876\u001b[0m        \u001b[35m0.4309\u001b[0m  0.0150\n",
      "      3       \u001b[36m0.9262\u001b[0m        \u001b[32m0.4191\u001b[0m        \u001b[35m0.3783\u001b[0m  0.0150\n",
      "      4       \u001b[36m0.9312\u001b[0m        \u001b[32m0.3663\u001b[0m        \u001b[35m0.3381\u001b[0m  0.0153\n",
      "      5       0.9312        \u001b[32m0.3258\u001b[0m        \u001b[35m0.3076\u001b[0m  0.0150\n",
      "Saved model to file 'exp1/model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "!python train.py --n_samples 1000 --output_file 'exp1/model.pkl' --lr 0.1 --max_epochs 5 \\\n",
    "  --device 'cuda' --module__hidden_units 50 --module__nonlin 'torch.nn.RReLU(0.1, upper=0.4)'\\\n",
    "  --callbacks__valid_acc__on_train --callbacks__valid_acc__name train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how you can even pass Python objects as arguments like `--module__nonlin 'torch.nn.RReLU(0.1, upper=0.4)'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easily hackable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made sure that skorch is as hackable as possible. On the neural net classes, look out for methods that start with `get_`, such as `get_loss`, or override the `train_step` itself. On the callbacks, look for methods that start with `on_`, such as `on_train_begin`. They always receive the associated `net` instance as the first parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_tweet(msg):\n",
    "    print(\"*tweet* {}\".format(msg))\n",
    "\n",
    "\n",
    "class TweetAccuracy(Callback):\n",
    "    def __init__(self, min_accuracy=0.99):\n",
    "        self.min_accuracy = min_accuracy\n",
    "\n",
    "    def on_train_end(self, net, **kwargs):\n",
    "        best_accuracy = max(net.history[:, 'valid_acc'])\n",
    "        if best_accuracy >= self.min_accuracy:\n",
    "            msg = \"Reached an accuracy of {:.4f}!!!\".format(best_accuracy)\n",
    "            send_tweet(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradAccNet(NeuralNetClassifier):\n",
    "    def __init__(self, *args, acc_steps=2, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.acc_steps = acc_steps\n",
    "\n",
    "    def get_loss(self, *args, **kwargs):\n",
    "        loss = super().get_loss(*args, **kwargs)\n",
    "        return loss / self.acc_steps  # normalize loss\n",
    "\n",
    "    def train_step(self, Xi, yi, **fit_params):\n",
    "        n_train_batches = len(self.history[-1, 'batches'])\n",
    "        step = self.train_step_single(Xi, yi, **fit_params)\n",
    "\n",
    "        if n_train_batches % self.acc_steps == 0:\n",
    "            self.optimizer_.step()\n",
    "            self.optimizer_.zero_grad()\n",
    "        return step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_acc_net = GradAccNet(MyModule, callbacks=[TweetAccuracy(min_accuracy=0.7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.4108\u001b[0m       \u001b[32m0.5532\u001b[0m        \u001b[35m0.3549\u001b[0m  0.1539\n",
      "      2        \u001b[36m0.3719\u001b[0m       \u001b[32m0.5912\u001b[0m        \u001b[35m0.3325\u001b[0m  0.1642\n",
      "      3        \u001b[36m0.3486\u001b[0m       \u001b[32m0.6227\u001b[0m        \u001b[35m0.3195\u001b[0m  0.5162\n",
      "      4        \u001b[36m0.3343\u001b[0m       \u001b[32m0.6537\u001b[0m        \u001b[35m0.3099\u001b[0m  0.8270\n",
      "      5        \u001b[36m0.3246\u001b[0m       \u001b[32m0.6722\u001b[0m        \u001b[35m0.3026\u001b[0m  0.1768\n",
      "      6        \u001b[36m0.3193\u001b[0m       \u001b[32m0.6907\u001b[0m        \u001b[35m0.2969\u001b[0m  0.2796\n",
      "      7        \u001b[36m0.3101\u001b[0m       \u001b[32m0.7046\u001b[0m        \u001b[35m0.2915\u001b[0m  0.3925\n",
      "      8        \u001b[36m0.3075\u001b[0m       \u001b[32m0.7161\u001b[0m        \u001b[35m0.2868\u001b[0m  0.1689\n",
      "      9        \u001b[36m0.3014\u001b[0m       \u001b[32m0.7276\u001b[0m        \u001b[35m0.2823\u001b[0m  0.1214\n",
      "     10        \u001b[36m0.2982\u001b[0m       \u001b[32m0.7336\u001b[0m        \u001b[35m0.2781\u001b[0m  0.5428\n",
      "*tweet* Reached an accuracy of 0.7336!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class '__main__.GradAccNet'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (dense): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (output): Linear(in_features=10, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_acc_net.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
