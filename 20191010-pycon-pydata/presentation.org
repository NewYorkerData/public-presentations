#+Title: skorch: A scikit-learn compatible neural network library that wraps PyTorch
#+Author: Benjamin Bossan
#+OPTIONS: toc:nil
#+REVEAL_TITLE_SLIDE: %t
#+MACRO: color @@html:<font color="$1">$2</font>@@
#+REVEAL_EXTRA_CSS: ./reveal.js/css/theme/league.css
#+REVEAL_EXTRA_CSS: ./local.css
#+OPTIONS: reveal_single_file:t
#+OPTIONS: num:nil

#+attr_html: :width 300px
#+CAPTION:
[[./assets/skorch_inv.svg]]
* Introduction
** About me
- one of the contributors to [[https://github.com/skorch-dev/skorch][skorch]]
- [[https://github.com/BenjaminBossan][BenjaminBossan]] on github
- head of data science Berlin at NewYorker (fashion company, not magazine)
** Become a deep data scientist now!
#+attr_html: :width 500px
#+CAPTION:
[[./assets/lecun_skorch.jpg]]
** Basic facts 1
#+attr_html: :width 800px
#+CAPTION: Github page as of Sep 29, 2019
[[./assets/skorch_github.png]]
** Basic facts 2
#+attr_html: :width 400px
#+CAPTION:
[[./assets/some_facts.svg]]
- mature: first commit July 2017
- 3 main contributors
- used in production
- many [[https://github.com/skorch-dev/skorch/tree/master/examples][examples]] and [[https://github.com/skorch-dev/skorch/tree/master/notebooks][notebooks]] in repository
- extensive docs: https://skorch.readthedocs.io
** Philosophy
- sklearn API
- hackable
- don't hide PyTorch
- don't reinvent the wheel
** Reuse existing concepts instead of reinventing the wheel
#+attr_html: :width 350px
#+CAPTION:
[[./assets/skorch_torch_sklearn.svg]]
** Advantages
- few new concepts to learn
- keeps complexity of code base down
- no lock in effect
  + easy to swap neural net with any sklearn estimator
  + easy to extract PyTorch module and use it without skorch
* Reduction of boilerplate code
** pure PyTorch
#+REVEAL_HTML: <div style="font-size: 50%;">
#+BEGIN_SRC python
import torch
from torch import nn

DEVICE = 'cuda'

class MyModule(nn.Module):
    # your normal module code

ds_train = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))
loader_train = DataLoader(ds_train, batch_size=256, shuffle=True)
ds_valid = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))
loader_valid = DataLoader(ds_valid, batch_size=256)
module = MyModule(num_units=50).to(DEVICE)
optimizer = torch.optim.SGD(module.parameters(), lr=0.02)
criterion = nn.NLLLoss()
template = "epoch: {} | loss train: {:.4f} | loss valid: {:.4f} | acc valid: {:.4f} | dur: {:.3f}"

for epoch in range(20):
    tic = time.time()
    losses_train = []
    for Xb, yb in loader_train:
        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)
        y_proba = module(Xb)
        loss = criterion(torch.log(y_proba), yb)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        losses_train.append(loss.item())
        
    losses_valid = []
    accuracy_valid = []
    for Xb, yb in loader_valid:
        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)
        y_proba = module(Xb)
        loss = criterion(torch.log(y_proba), yb)
        optimizer.step()
        optimizer.zero_grad()
        losses_valid.append(loss.item())
        accuracy_valid.append(accuracy_score(yb.cpu().numpy(), y_proba.argmax(1).cpu().numpy()))
        
    toc = time.time() - tic
    print(template.format(
        epoch + 1, np.mean(losses_train), np.mean(losses_valid), np.mean(accuracy_valid), toc))
#+END_SRC
#+REVEAL_HTML: </div>
** same in skorch
#+BEGIN_SRC python
from torch import nn
from skorch import NeuralNetClassifier

class MyModule(nn.Module):
    # your normal module code

net = NeuralNetClassifier(
    MyModule,
    module__num_units=50,
    max_epochs=20,
    lr=0.02,
    batch_size=256,
    iterator_train__shuffle=True,
    device='cuda',
)
net.fit(X, y)
#+END_SRC
** useful print log
#+attr_html: :width 400px
#+CAPTION:
[[./assets/skorch_print_log.png]]
* sklearn compatibility
** Monitor any sklearn metric
#+BEGIN_SRC python
from skorch.callbacks import EpochScoring
from sklearn.metrics import roc_auc_score

auc = EpochScoring(
    scoring=roc_auc_score,  # <-- just passing 'roc_auc' would also work
    lower_is_better=False,
)

net = NeuralNetClassifier(
    ...
    callbacks=[auc],
)
net.fit(X, y)
#+END_SRC
Use any of the dozens of metrics that sklearn offers or easily write
your own scoring functions.
** full sklearn API
#+BEGIN_SRC python
from sklearn.base import clone
from sklearn.model_selection import cross_validate

net.fit(X, y)
net.predict(X)
net.predict_proba(X)
net.get_params()
net.set_params(...)
clone(net)
cross_validate(net, X, y)
...
#+END_SRC
** sklearn Pipelines
#+BEGIN_SRC python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

pipe = Pipeline([
    ('scale', StandardScaler()),
    ('net', net),
])

pipe.fit(X, y)
y_pred = pipe.predict(X)
y_proba = pipe.predict_proba(X)
#+END_SRC
** supports pickling
#+BEGIN_SRC python
import pickle

pipe = Pipeline([
    ('scale', StandardScaler()),
    ('net', net),
])

pipe.fit(X, y)

with open('my_pipeline.pickle', 'wb') as f:
    pickle.dump(pipe, f)
#+END_SRC
Training on GPU, loading on CPU is no problem.
** GridSearchCV et al.
#+BEGIN_SRC python
from sklearn.model_selection import GridSearchCV

class MyModule(nn.Module):
    def __init__(self, num_units=10, dropout=0.5):
        super().__init__()

        self.dense = nn.Linear(20, num_units)
        self.dropout = nn.Dropout(dropout)
        self.output = nn.Linear(num_units, 2)

    def forward(self, X, **kwargs):
        ...

params = {
    'max_epochs': [10, 20],
    'optimizer__momentum': [0.0, 0.9],
    'module__num_units': [10, 20, 50],
    'module__dropout': [0, 0.5],
}
search = GridSearchCV(net, params).fit(X, y)
#+END_SRC
** Grid search everything!
You can grid search the parameters of almost everything:

#+REVEAL_HTML: <div style="font-size: 75%;">
| component    | examples                                                           |
|--------------+--------------------------------------------------------------------|
| ~NeuralNet~  | ~lr, max_epochs, batch_size~                                       |
| ~module~     | ~module__num_units, module__nonlin~                                |
| ~optimizer~  | ~optimizer__momentum, optimizer__nesterov~                         |
| ~criterion~  | ~criteron__reduction, criterion__ignore_index~                     |
| ~DataLoader~ | ~iterator_train__shuffle, iterator_valid__num_workers~             |
| ~callbacks~  | ~callbacks__print_log__sink, callbacks__mycallback__any_parameter~ |
#+REVEAL_HTML: </div>
** swap out skorch estimator with any sklearn estimator
#+BEGIN_SRC python
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

pipe = Pipeline([
    ('scale', StandardScaler()),
    ('model', net),
])
params = {'model': [net, LogisticRegression(), KNeighborsClassifier()]}
search = GridSearchCV(pipe, params)
search.fit(X, y)
#+END_SRC
** distributed GridSearchCV
#+BEGIN_SRC python
from dask.distributed import Client
from joblib import parallel_backend

client = Client('127.0.0.1:8786')

search = GridSearchCV(...)
with parallel_backend('dask'):
    search.fit(X, y)
#+END_SRC
* Many more additions
** Save ~state_dict~ for better compatibility
#+BEGIN_SRC python
from skorch.callbacks import Checkpoint

cp = Checkpoint(monitor='valid_loss_best', dirname='exp1')
net = NeuralNetClassifier(..., callbacks=[cp])
net.fit(X, y)  # Checkpoint saves each time valid lost improves
net.save_params(
    f_params='exp1/mynet.pt',  # <- state dict of module
    f_optimizer='exp1/myoptimizer.pt',  # <- state dict of optimizer
)
#+END_SRC
** Handling of different data formats
- numpy arrays
- PyTorch Datasets (most)
- dict or list of arrays
- pandas DataFrames
- scipy sparse CSR matrices
** Callbacks
- learning rate schedulers
- scoring functions (custom or sklearn metrics)
- early stopping
- checkpointing
- parameter freezing
- tensorboard
- ...
** Helper for command line usage
*** Extend training script with a few lines to transform it into a useful CLI
#+BEGIN_SRC python
import fire
from skorch.helper import parse_args

# training code goes here

def main(**kwargs):
    X, y = ...
    model = NeuralNetClassifier(...)
    # important: wrap the model with the parsed arguments
    model = parse_args(kwargs)(model)
    model.fit(X, y)

if __name__ == '__main__':
    fire.Fire(main)
#+END_SRC
No longer write dozen of lines of argument parsing and still forget
half of the arguments.
*** Get help for free
#+BEGIN_SRC shell
$ python train.py --help
<NeuralNetClassifier> options:
  --module : torch module (class or instance)
    A PyTorch :class:`~torch.nn.Module`. In general, the
    uninstantiated class should be passed, although instantiated
    modules will also work.
  --criterion : torch criterion (class, default=torch.nn.NLLLoss)
    Negative log likelihood loss. Note that the module should return
    probabilities, the log is applied during ``get_loss``.
  --optimizer : torch optim (class, default=torch.optim.SGD)
    The uninitialized optimizer (update rule) used to optimize the
    module
  --lr : float (default=0.01)
    Learning rate passed to the optimizer. You may use ``lr`` instead
    of using ``optimizer__lr``, which would result in the same outcome.
  ...

<MyModule> options:
  --module__hidden_units : int (default=30)
    Number of units in hidden layers.
  --module__nonlin : torch.nn.Module instance (default=torch.nn.ReLU())
    Non-linearity to apply after hidden layers.
  ...
#+END_SRC
*** All parameters can now be accessed from the command line
#+BEGIN_SRC shell
$ python train.py --lr 0.1 --max_epochs 5 --device 'cuda' \
  --module__hidden_units 50 --module__nonlin 'torch.nn.RReLU(0.1, upper=0.4)' \
  --callbacks__valid_acc__on_train --callbacks__valid_acc__name 'new_name'
#+END_SRC
* Easily hackable
** Methods designed for easy overriding
#+BEGIN_SRC python
class MyNet(NeuralNet):
    def get_loss(...):
    def get_iterator(...):
    def train_step(...):

class MyCallback(Callback):
    def on_train_begin(...):
    def on_epoch_begin(...):
    def on_batch_end(...)
    def on_grad_computed(...):
#+END_SRC
** Example: write your own callback
After training ends, send a tweet with best validation accuracy
#+BEGIN_SRC python
from skorch.callbacks import Callback

def send_tweet(msg):
    ...

class TweetAccuracy(Callback):
    def __init__(self, min_accuracy=0.99):
        self.min_accuracy = min_accuracy

    def on_train_end(self, net, **kwargs):
        best_accuracy = max(net.history[:, 'valid_acc'])
        if best_accuracy >= self.min_accuracy:
            msg = "Reached an accuracy of {:.4f}!!!".format(best_accuracy)
            send_tweet(msg)
#+END_SRC
** Example: implement gradient accumulation
#+BEGIN_SRC python
class GradAccNet(NeuralNetClassifier):
    def __init__(self, *args, acc_steps=2, **kwargs):
        super().__init__(*args, **kwargs)
        self.acc_steps = acc_steps

    def get_loss(self, *args, **kwargs):
        loss = super().get_loss(*args, **kwargs)
        return loss / self.acc_steps  # normalize loss

    def train_step(self, Xi, yi, **fit_params):
        n_train_batches = len(self.history[-1, 'batches'])
        step = self.train_step_single(Xi, yi, **fit_params)

        if n_train_batches % self.acc_steps == 0:
            self.optimizer_.step()
            self.optimizer_.zero_grad()
        return step
#+END_SRC
* Questions?
skorch: https://github.com/skorch-dev/skorch
-----
#+REVEAL_HTML: <div style="font-size: 65%;">
get the presentation here: https://git.io/JeC0C

main contributors:
- [[https://github.com/ottonemo][ottonemo]]
- [[https://github.com/thomasjpfan][thomasjpfan]]
- [[https://github.com/BenjaminBossan][benjaminbossan]]

-----

Open positions: https://jobs.newyorker.de
- Senior Data Scientist in Berlin
- Senior Python Software Developer in Berlin

#+attr_html: :width 100px
[[./assets/NY_RGB.svg]]

#+REVEAL_HTML: </div>
